\section{Generative Models}
\subsection{Unsupervised Learning}
\begin{itemize}
	\item Supervised Learning
	\begin{itemize}
		\item Data $(x, y)$ : $x$ is data, $y$ is label
		\item Goal: Learn a \textbf{function} to map $x \rightarrow y$
		\item Examples: Classification, regression, object detection, semantic segmentation, image captioning, etc.
	\end{itemize}
	\item Unsupervised Learning
	\begin{itemize}
		\item Data $x$: Just data, no labels
		\item Goal: Learn some underlying hidden \textbf{structure} of the data
		\item Examples: Clustering, dimensionality reduction, feature learning, density estimation, etc.
		\item Training data is cheap (no label)
		\item Understand structure of visual world
	\end{itemize}
\end{itemize}

\subsection{Generative Models}
\begin{itemize}
	\item Given training data, generate new samples from same distribution
	\item Training data $\sim p_{data}(x)$, Generated samples $\sim p_{model}(x)$
	\item Want to learn $p_{model}(x)$ similar to $p_{data}(x)$
	\item Addresses density estimation (core problem in unsupervised learning)
	\begin{itemize}
		\item Explicit density estimation: explicitly define and solve for $p_{model}(x)$
		\item Implicit density estimation: learn model that can sample from $p_{model}(x)$ without explicitly defining it
	\end{itemize}
	\item Why?
	\begin{itemize}
		\item Realistic samples for artwork, super-resolution, colorization, etc.
		\item Generative models of time-series data can be used for simulation and planning
		\item Training generative models can also enable inference of latent representations that can be useful as general features
	\end{itemize}
\end{itemize}

\subsection{PixelRNN and PixelCNN}
\begin{itemize}
	\item Fully visible belief network, explicit density model
	\item Use chain rule to decompose likelihood of an image $x$ into product of 1D distributions
	$$p(x) = \prod_{i=1}^n p(x_i\mid x_1, \dots, x_{i-1})$$
	Then maximize the likelihood of training data
	\item \textbf{PixelRNN}
	\begin{itemize}
		\item Generate image pixels starting from corner
		\item Dependency on previous pixels modeled using and RNN (LSTM)
		\item Sequential generation - slow
	\end{itemize}
	\item \textbf{PixelCNN}
	\begin{itemize}
		\item Still generate image pixels starting from corner
		\item Dependency on previous pixels now modeled using a CNN over context region
		\item Outputing a distribution over pixel values at each location of image
		\item Training: maximize likelihood of training images, softmax loss at each pixel
		\item Training time is faster than PixelRNN - parallelize convolutions since context region values are known from training images
		\item Generation must still proceed sequentially - slow
	\end{itemize}
	\item Pros
	\begin{itemize}
		\item Can explicitly compute likelihood $p(x)$
		\item Explicit likelihood of training data gives good evaluation metric
		\item Good samples
	\end{itemize}
	\item Cons
	\begin{itemize}
		\item Sequential generation is slow
	\end{itemize}
\end{itemize}

\subsection{Autoencoders}
\begin{itemize}
	\item Unsupervised approach for learning a lower-dimensional feature representation from unlabled training data
	\item Input Data $\rightarrow$ Features $\rightarrow$ Reconstructed input data
	$$x\overset{Encoder}{\longrightarrow} z \overset{Decoder}{\longrightarrow} \widehat{x}$$
	\item Encoder, Decoder: Originally - linear + nonlinearity (sigmoid), later - Deep, fully connected layer, ReLU CNN
	\item $z$ is usually smaller than $x$ due to dimensionality reduction - we want features to capture meaningful factors of variation in data
	\item Train so that features can be used to reconstruct original data (``\textbf{Autoencoding}" - encoding itself)
	\item $L_2$ loss function $\norm{x-\widehat{x}}^2$ (Doesn't use labels)
	\item After training, throw away the decoder
	\item Encoder can be used to initialize a \textbf{supervised} model and add a classifier at the end
	\item Able to use unlabeled training data to try and learn good general feature representation, use this to initialize supervised learning problem where we only have small data
	\item Features $z$ capture factors of variation in training data. Can we generate new images from an autoencoder?
\end{itemize}

\subsection{Variational Autoencoders (VAE)}
\begin{itemize}
	\item VAEs define intractable density function with latent $z$
	$$p_\theta (x) = \int p_\theta (z) p_\theta (x\mid z)\, dz$$
	cannot optimize directly, we have to derive and optimize lower bound on likelihood instead
	\item Probabilistic spin on autoencoders - will let us \textit{sample} from the model to \textit{generate} data
	\item Assume training data $\{x^{(i)}\}_{i=1}^N$ is generated from underlying unobserved (latent) representation $z$
	\item \textbf{Intuition}: $x$ is an image, $z$ is latent factors used to generate $x$ - attributes, orientation, etc.
	\item Sample from true prior $p_{\theta^\ast}(z)$, then sample from true conditional $p_{\theta^\ast}(x\mid z^{(i)})$
	\item We want to estimate the true parameters $\theta^\ast$ of this generative model
	\item How should we represent this model?
	\begin{itemize}
		\item Choose prior $p(z)$ to be simple, such as Gaussian (Reasonable for latent attributes)
		\item Conditional $p(x\mid z)$ is much more complex (need this to generate image) - represent with neural network
	\end{itemize}
	\item How to train the model?
	\begin{itemize}
		\item Remember the strategy for training generative models from FVBN\footnote{Fully Visible Belief Networks} - learn model parameters to maximize likelihood of training data
	\end{itemize}
	\item Intractability
	\begin{itemize}
		\item Data likelihood: $p_\theta(x) = \ds \int p_\theta(z) p_\theta(x\mid z)\,dz$
		\item $p_\theta(z)$: Simple Gaussian prior
		\item $p_\theta(x\mid z)$: Decoder neural network
		\item Intractable to compute $p(x\mid z)$ for every $z$
		\item Posterior density also intractable: $p_\theta (z\mid x) = p_\theta(x\mid z)p_\theta(z)/ p_\theta(x)$
	\end{itemize}
	\item Solution
	\begin{itemize}
		\item In addition to decoder network modeling $p_\theta (x\mid z)$, define an additional encoder network $q_\phi(z\mid x)$ that approximates $p_\theta(z\mid x)$
		\item This allows us to derive a lower bound on the data likelihood that is tractable, which we can optimize
	\end{itemize}
	\item Since we're modeling probabilistic generation of data, encoder and decoder networks are probabilistic (Mean and diagonal covariance)
	\begin{itemize}
		\item $x$ $\rightarrow$ (Encoder network $q_\phi (z\mid x)$) $\mu_{z\mid x}$, $\Sigma_{z\mid x}$ - Sample $z$ from $z\mid x \sim \mc{N}(\mu_{z\mid x}, \Sigma_{z\mid x})$
		\item $z$ $\rightarrow$ (Decoder network $p_\theta (x\mid z)$) $\mu_{x\mid z}$, $\Sigma_{x\mid z}$ - Sample $x\mid z$ from $x\mid z \sim \mc{N}(\mu_{x\mid z}, \Sigma_{x\mid z})$
	\end{itemize}
	\item Encoder and decoder networks are also called recognition/inference and generation networks
	\item Data likelihood
	$$\begin{aligned}
		\log p_\theta\left(x^{(i)}\right) &= \mathrm{E}_{z\sim q_\phi(z\mid x^{(i)})}\left[\log p_\theta \left(x^{(i)}\right)\right] \qquad \left(p_\theta\left(x^{(i)}\right) \text{does not depend on } z\right)\\
		&= \mathrm{E}_{z} \left[\log\frac{p_\theta(x^{(i)} \mid z) p_\theta(z)}{p_\theta(z\mid x^{(i)})}\right] \qquad \left(\text{Bayes' Rule} ???\right)\\
		&= \mathrm{E}_{z}\left[\log p_\theta(x^{(i)} \mid z)\right] - \mathrm{E}_{z}\left[\log\frac{q_\phi\left(z\mid x^{(i)}\right)}{p_\theta(z)}\right] + \mathrm{E}_{z}\left[\log\frac{q_\phi\left(z\mid x^{(i)}\right)}{p_\theta\left(z\mid x^{(i)}\right)}\right] \\
		&=\mathrm{E}_{z}\left[\log p_\theta(x^{(i)} \mid z)\right] \\&\qquad - D_{KL}\left(q_\phi\left(z\mid x^{(i)}\right) \parallel p_\theta(z) \right) + D_{KL}\left(q_\phi\left(z\mid x^{(i)}\right) \parallel p_\theta\left(z\mid x^{(i)}\right)\right)\\
	\end{aligned}$$
	\begin{itemize}
		\item $\mathrm{E}_{z}\left[\log p_\theta(x^{(i)} \mid z)\right]$: Decoder network gives $p_\theta(x\mid z)$, can compute estimate of this term through sampling
		\item $- D_{KL}\left(q_\phi\left(z\mid x^{(i)}\right) \parallel p_\theta(z) \right)$: This KL term (between Gaussians for encoder and $z$ prior) has nice closed-form solution
		\item $D_{KL}\left(q_\phi\left(z\mid x^{(i)}\right) \parallel p_\theta\left(z\mid x^{(i)}\right)\right)$: $p_\theta(z\mid x)$ is intractable, can't compute, but we know that KL divergence term is always $\geq 0$
		\item Since last term is always $\geq 0$...
	\end{itemize}
	\item \textbf{Tractable Lower Bound} which we can take gradient of and optimize (differentiable)
	$$\mc{L}\left(x^{(i)}, \theta, \phi \right) =\mathrm{E}_{z}\left[\log p_\theta(x^{(i)} \mid z)\right] - D_{KL}\left(q_\phi\left(z\mid x^{(i)}\right) \parallel p_\theta(z) \right) $$
	\begin{itemize}
		\item $\mathrm{E}_{z}\left[\log p_\theta(x^{(i)} \mid z)\right]$: Trying to do a good reconstruction of input data
		\item $D_{KL}\left(q_\phi\left(z\mid x^{(i)}\right) \parallel p_\theta(z) \right)$: Want this KL term to be small, which means that we want top make approximate posterior distribution close to the prior distribution 
	\end{itemize}
	\item $\therefore \log p_\theta\left(x^{(i)}\right) \geq \mc{L}\left(x^{(i)}, \theta, \phi \right)$ (Variational lower bound - ELBO\footnote{Evidence Lower Bound})
	\item Training: \textbf{Maximize} lower bound
	$$\ds \theta^\ast, \phi^\ast = \arg\max_{\theta, \phi} \sum_{i=1}^N \mc{L}\left(x^{(i)}, \theta, \phi \right)$$
	\begin{itemize}
		\item Calculate the KL term from the $z\mid x$ that came from the encoder network
		\item And calculate the expectation from the $x\mid z$ which came from the output of decoder
		\item For every minibatch of input data, compute this forward pass, and backprop
	\end{itemize}
	\item Generating Data - Use decoder network, sample $z$ from prior
	\begin{itemize}
		\item Diagonal prior on $z$: independent latent variables
		\item Different dimensions of $z$ encode interpretable factors of variation
		\item Also good feature representation that can be computed using $q_\phi(z\mid x)$
	\end{itemize}
	\item \textbf{Summary}
	\begin{itemize}
		\item Probabilistic spin to traditional autoencoders allows generating data
		\item For training, defines an intractable density, derives and optimizes a (variational) lower bound
	\end{itemize}
	\item Pros
	\begin{itemize}
		\item Principled approach to generative models
		\item Allows inference of $q(z\mid x)$, can be useful feature representation for other tasks
	\end{itemize}
	\item Cons
	\begin{itemize}
		\item Maximizes lower bound of likelihood is ok, but not as good evaluations as PixelRNN/PixelCNN
		\item Samples blurrier and lower quality compared to SOTA GANs
	\end{itemize}
	\item Areas of Research
	\begin{itemize}
		\item More flexible approximations (richer approximate posterior instead of diagonal Gaussian)
		\item Incorporating structure in latent variables
	\end{itemize}
\end{itemize}

\subsection{Generative Adversarial Networks (GAN)}
\begin{itemize}
	\item What if we give on explicitly modeling density, and just want ability to sample?
	\item Take a game-theoretic approach - learn to generate from training distribution through 2 player game
	\item Problem: Want to sample from complex, high-dimensional training distribution
	\item Solution: Sample from a simple distribution and learn transformation to training distribution
	\item Training GANs
	\begin{itemize}
		\item \textbf{Generator network}: Try to fool the discriminator by generating real-looking images
		\item \textbf{Discriminator network}: Try to distinguish between real and fake images
		\item Train jointly in \textbf{minimax game}
	\end{itemize}
	\item Objective function:
	$$\min_{\theta_g} \max_{\theta_d}\left( \mathrm{E}_{x\sim p_{data}}\left[\log D_{\theta_d}(x)\right] + \mathrm{E}_{z\sim p(z)} \left[\log(1-D_{\theta_d}(G_{\theta_g}(z)))\right]\right)$$
	\begin{itemize}
		\item Discriminator ($\theta_d$) wants to \textbf{maximize objective} such that $D(x)$ is close to 1 (real image) and $D(G(z))$ is close to 0 (fake image)
		\item Generator ($\theta_g$) wants to \textbf{minimize objective} such that $D(G(z))$ is close to 1 (discriminator is fooled into thinking generated $G(z)$ is real)
	\end{itemize}
	\item Alternate between:
	\begin{itemize}
		\item \textbf{Gradient ascent} on discriminator $$\max_{\theta_d}\left( \mathrm{E}_{x\sim p_{data}}\left[\log D_{\theta_d}(x)\right] + \mathrm{E}_{z\sim p(z)} \left[\log(1-D_{\theta_d}(G_{\theta_g}(z)))\right]\right)$$
		\item \textbf{Gradient ascent} on generator\footnote{$\log(1-x)$ doesn't work well with gradient descent - low gradient on bad samples, high gradient on good samples} $$\max_{\theta_g} \left(\mathrm{E}_{z\sim p(z)} \left[\log(D_{\theta_d}(G_{\theta_g}(z)))\right]\right)$$
		(Instead of minimizing likelihood of discriminator being correct, now maximize likelihood of discriminator being wrong)
	\end{itemize}
	\item GAN training algorithm
	\item After training, use generator network to generate new images
	\item GANs: Convolutional Architectures (Radford et al, ICLR 2016)
	\item GANs: Interpretable Vector Math
	\item Pros
	\begin{itemize}
		\item Beautiful, SOTA samples
	\end{itemize}
	\item Cons
	\begin{itemize}
		\item Trickier, more unstable to train
		\item Can't solve inference queries such as $p(x)$, $p(z\mid x)$
	\end{itemize}
	\item Areas of Research
	\begin{itemize}
		\item Better loss functions, more stable training (Wasserstein GAN, LSGAN)
		\item Conditional GANs, GANs or all kinds of applications
	\end{itemize}
\end{itemize}
